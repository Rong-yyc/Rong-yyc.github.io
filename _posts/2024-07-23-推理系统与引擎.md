# 推理系统

推理系统是一个专门用于部署神经网络模型，执行推理预测任务的 AI 系统。

通过推理系统，可以将神经网络模型部署到云端或者边缘端，并服务和处理用户的请求。因此，推理系统也需要应对模型部署和服务生命周期中遇到的挑战和问题。

## AI生命周期

神经网络模型的生命周期（Life Cycle）最核心的主要由数据准备、模型训练推理以及模型部署三个阶段组成。

### 数据准备

对于模型的训练可以采用传统的中心化训练方式，将所有的数据都集中存储在一个中心服务器上。然而，随着数据量的增加，将所有数据集中存储会导致显著的通信和隐私问题。

因此，也可以采用将数据分散存储在不同的地方，同时通过联合学习的方式进行模型训练的去中心化分布式训练。

### 训练和推理

因为推理过程通常是在生产环境中执行的，因此性能和效率至关重要。而测试则是评估模型性能的过程，通常包括在一组已知的测试数据上运行训练好的模型，并评估其在测试数据上的表现，以了解模型的准确性、精确度、召回率等性能指标。

### 模型部署

推理系统一般可以部署在云或者边缘。云端部署的推理系统更像传统 Web 服务，在边缘侧部署的模型更像手机应用和 IOT 应用系统。

首先，通过剪枝（减少不必要的参数和连接）、量化（减少数值精度以减小模型大小和计算量）以及蒸馏（利用更小的模型传递主模型的知识）等技术对模型进行优化和压缩，用于提高部署阶段的效率和性能。

其次，根据具体需求和平台限制，选择适合的推理引擎。常用的推理引擎如 TensorRT、OpenVINO、ONNX Runtime 等针对不同硬件设备进行优化，提供高效的模型推理能力。

然后，在部署中可能涉及创建 API 接口、配置服务器、设置数据传输和存储等。在部署后，持续监控模型的性能，并根据需要进行优化。这可能包括调整模型参数、更新推理引擎版本、优化硬件资源分配等。

## 推理场景

在深度学习中，推理任务相较于训练任务确实具有一系列独特的新特点与挑战。具体来说，这些特点和挑战包括：

1. **长期运行服务需求：**神经网络模型在推理阶段通常被部署为长期运行的服务，如互联网服务。这类服务对请求的响应有严格的低延迟（Low Latency）和高吞吐（High Throughput）要求。
2. **推理资源约束更为苛刻：**与训练阶段相比，推理阶段通常面临更为严格的资源约束，如更小的内存、更低的功耗等。这是因为推理任务通常需要在各种设备上运行，如手机、嵌入式设备等，这些设备的资源远小于数据中心的商用服务器。
3. **不需要反向传播和梯度下降：**在推理阶段，模型不再需要反向传播和梯度下降来进行参数更新。因此，为了提升推理速度和效率，可以采取一些牺牲数据精度的策略，如量化、稀疏性等。这些策略可以在一定程度上换取推理性能的提升。
4. **部署设备型号多样性：**由于神经网络模型需要在各种设备上进行推理，但是这些设备的型号和配置可能各不相同，为了在各种设备上实现高效的推理，因此需要进行多样化的定制和优化。

在推理阶段，低延迟是更为关键的性能指标，而高吞吐量虽然在训练期间是重要的，但在推理时则相对次要。

# 推理系统优化目标

## 灵活性（Flexibility）

包括支持多种框架，为构建不同应用提供灵活性，以扩大神经网络模型部署的覆盖场景，增强部署的生产力。

为了支持多种框架，可以利用模型转换工具，将不同框架的模型转换为一种通用的中间表示。ONNX（Open Neural Network Exchange）是一个广泛采用的中间表示格式，它允许开发者在不同 AI 框架之间转换和部署模型。

## 延迟（Latency）

低延迟是推理系统的重要考量指标，它直接影响用户查询后获取推理结果的等待时间。

在实际操作中，会运用模型压缩、剪枝、量化等技术降低模型的复杂度和大小，从而减少推理时间。此外，还会对数据预处理和后处理步骤进行优化，以降低整个推理管道的延迟。同时，分布式系统设计，如负载均衡、数据局部性优化等策略，也能有效提升系统整体性能和降低延迟。最后，实施 SLA 监控和告警系统，可以实时监测系统的性能和延迟，确保系统达到预定的性能目标。

## 吞吐量（Throughputs）

高吞吐量是确保系统能应对大量服务请求的关键。

在实际操作中，可以采取多种策略来提升系统的吞吐量。首先，利用多线程、多进程或分布式计算技术，系统可以并行处理多个推理请求，显著提高处理能力。其次，服务网格（如 Istio、Linkerd）和分布式系统设计能够有效管理和优化服务间的通信，进一步提升系统整体的吞吐量。此外，采用异步处理和消息队列技术可以解耦系统的不同组件，避免因等待某些操作完成而造成系统阻塞，从而提高吞吐量。

## 高效率（Efficiency）

高效率是降低推理服务成本、提升系统性能的核心。系统应实现高效执行，低功耗使用 GPU 和 CPU，通过降本增效来优化推理服务的整体成本效益。

为了实现这一目标，可以采用动态电压与频率调整（DVFS）、低功耗模式等技术，根据实时计算需求动态调整硬件功耗。此外，研究和开发更高效的算法也是提升效率的重要途径

## 扩展性（Scalability）

可扩展性是应对不断增长的用户或设备需求的基础。

借助底层 Kubernetes 部署平台，用户可以便捷地配置和自动部署多个推理服务副本，并通过前端负载均衡服务达到高扩展性和提升吞吐量，进一步增强推理服务的可靠性。另外，云计算平台如 AWS、Azure、谷歌 Cloud 等提供了弹性的计算、存储和网络服务，这些服务可以根据需求快速扩展资源。使用负载均衡器（如 Ingress 控制器）可以分发进入网络的流量，确保请求均匀分配到不同的服务实例上，从而提高系统的吞吐量和可靠性。

## 可靠性（Reliability）

可靠性是保障推理服务持续运行和用户体验的关键。

为了实现高可靠性，可以部署多个服务副本，并在多个数据中心或云区域进行跨地域部署；在设计系统时，应考虑故障转移机制；面对高负载或潜在的故障情况，系统应通过限流和降级策略来保护自身；使用健康检查机制（如 Kubernetes 的 Liveness 和 Readiness 探针）来监控应用的健康状态，并在检测到问题时自动重启或替换有问题的容器或服务。等等

# 推理系统 vs 推理引擎

推理系统的构建涉及到多个核心环节，以确保请求与响应的高效处理、资源的高效调度、推理引擎的灵活适配、模型版本的有效管理、服务的健康监控以及边缘推理芯片与代码编译的优化。推理系统需要采用高效的传输、序列化、压缩与解压缩机制，以确保数据传输的效率和性能，进而实现低延迟、高吞吐的服务。

推理引擎是系统的核心组件，它负责将请求映射到相应的模型作为输入，并在运行时调度神经网络模型的内核进行多阶段处理。当系统部署在异构硬件或多样化的环境中时，推理引擎还可以利用编译器进行代码生成与内核算子优化，使模型能够自动转换为特定平台的高效可执行机器码，进一步提升推理性能。推理引擎本身可视为一种基础软件，它提供了一组丰富的 API，使得开发者能够在多种特定平台（如 CPU、GPU 和 VPU）上高效执行推理任务。