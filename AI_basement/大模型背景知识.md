# 神经网络基本组件

- **神经元（Neuron）**：神经网络的基本组成单元，模拟生物神经元的功能，接收输入信号并产生输出，这些神经元在一个神经网络中称为模型权重。
- **激活函数（Activation Function）**：用于神经元输出非线性化的函数，常见的激活函数包括 Sigmoid、ReLU 等。
- **模型层数（Layer）**：神经网络由多个层次组成，包括输入层、隐藏层和输出层。隐藏层可以有多层，用于提取数据的不同特征。
- **前向传播（Forward Propagation）**：输入数据通过神经网络从输入层传递到最后输出层的过程，用于生成预测结果。
- **反向传播（Backpropagation）**：通过计算损失函数对网络参数进行调整的过程，以使网络的输出更接近预期输出。
- **损失函数（Loss Function）**：衡量模型预测结果与实际结果之间差异的函数，常见的损失函数包括均方误差和交叉熵。
- **优化算法（Optimization Algorithm）**：用于调整神经网络参数以最小化损失函数的算法，常见的优化算法包括梯度下降，自适应评估算法（Adaptive Moment Estimation）等，不同的优化算法会影响模型训练的收敛速度和能达到的性能。

# 计算范式：权重求和

神经网络中 90%的计算量都是在做乘加（multiply and accumulate, MAC）的操作，也称为权重求和。

## 矩阵乘的优化

1. **基本的循环优化**
2. **分块矩阵乘法（Blocked Matrix Multiplication）**
3. **SIMD 指令优化**
4. **SIMT 多线程并行化**
5. **算法改进**

在具体的 AI 芯片或其它专用芯片里面，对矩阵乘的优化实现主要就是减少指令开销，可以表现为两个方面：

1. **让每个指令执行更多的 MACs 计算。**比如 CPU 上的 SIMD/Vector 指令，GPU 上的 SIMT/Tensor 指令，NPU 上 SIMD/Tensor,Vector 指令的设计。
2. **在不增加内存带宽的前提下，单时钟周期内执行更多的 MACs。**比如英伟达的 Tensor Core 中支持低比特计算的设计，对每个 cycle 执行 512bit 数据的带宽前提下，可以执行 64 个 8bit 的 MACs，大于执行 16 个 32bit 的 MACs。

# 主流的网络模型结构

1. 全连接 Fully Connected Layer
2. 卷积层 Convolutional Layer
3. 循环网络 Recurrent Layer
4. 注意力机制 Attention Layer

# 模型量化与压缩

## 量化

模型量化是指通过减少神经模型权重表示或者激活所需的比特数来将高精度模型转换为低精度模型。

量化的类型分对称量化和非对称量化。

模型量化本质上就是将神经网络模型中权重和激活值从高比特精度（FP32）转换为低比特精度（如 INT4/INT8 等）的技术，用来加速模型的部署和集成，降低硬件成本，为实际应用带来更多可能性

## 压缩

网络剪枝则是研究模型权重中的冗余，尝试在不影响或者少影响模型推理精度的条件下删除/修剪冗余和非关键的权重。

模型剪枝是一种有效的模型压缩方法，通过对模型中的权重进行剔除，降低模型的复杂度，从而达到减少存储空间、计算资源和加速模型推理的目的。

## 数据精度格式

目前神经网络中的常用的类型有 FP32, TF32, FP16, BF16 这四种类型。

降低比特位宽其实就是降低数据的精度，对于 AI 芯片来说，降低比特位宽可以带来如下好处：

1. 降低 MAC 的输入和输出数据位宽，能够有效减少数据的搬运和存储开销。更小的内存搬移带来更低的功耗开销。
2. 减少 MAC 计算的开销和代价，比如，两个 int8 数据类型的相乘，累加和使用 16bit 位宽的寄存器即可，而 FP16 数据类型的相乘，累加和需要设计 32 位宽的寄存器。8bit 和 16bit 计算对硬件电路设计的复杂度影响也很大。

# 神经网络规模衡量指标

**Params 网络参数量**

对于输入为 w×h×Ci 的输入图像，卷积核大小为 k×k，得到输出的特征图大小为 W×H×Co 的卷积操作，其参数量为：Params=(k×k×Ci+1)×Co。

**FLOPs 浮点运算数**

对于输入为 w×h×Ci 的输入图像，卷积核大小为 k×k，得到输出的特征图大小为 W×H×Co 的卷积操作，其浮点运算数为：FLOPs=W×H×(k×k×Ci+1)×Co。

一般来说，网络模型参数量和浮点运算数越小，模型的速度越快，但是衡量模型的快慢不仅仅是参数量和计算量的多少，还有内存访问的次数多少相关，也就是和网络结构本身相关。

# 模型轻量化方法

1. **减少内存空间的设计**
2. **减少通道数的设计**
3. **减少卷积核个数的设计**